{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 3.5194811820983887\n",
      "(1600000, 2)\n",
      "(359, 2)\n"
     ]
    }
   ],
   "source": [
    "tweets=[]\n",
    "start= time.time()\n",
    "train_data= pd.read_csv('data/training.csv',names = ['Type' , 'id', 'date', 'query','user', 'tweet'], encoding='latin-1')\n",
    "test_data= pd.read_csv('data/test.csv',names = ['Type' , 'id', 'date', 'query','user', 'tweet'], encoding='latin-1')\n",
    "test_data = test_data.drop(['id','date','query','user'], axis=1)\n",
    "train_data= train_data.drop(['id','date','query','user'], axis=1)\n",
    "test_data = test_data.drop(test_data[test_data['Type']==2].index)\n",
    "# print(train_data[:5])\n",
    "# print(train_data[:5])\n",
    "print('time',time.time()-start)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  words  word_count\n",
      "Type                                                               \n",
      "0     [@switchfoot, http://twitpic.com/2y1zl, -, Aww...          19\n",
      "0     [is, upset, that, he, can't, update, his, Face...          21\n",
      "0     [@Kenichan, I, dived, many, times, for, the, b...          18\n",
      "0     [my, whole, body, feels, itchy, and, like, its...          10\n",
      "0     [@nationwideclass, no,, it's, not, behaving, a...          21\n",
      "                                                  words  word_count\n",
      "Type                                                               \n",
      "4     [Just, woke, up., Having, no, school, is, the,...          11\n",
      "4     [TheWDB.com, -, Very, cool, to, hear, old, Wal...          11\n",
      "4     [Are, you, ready, for, your, MoJo, Makeover?, ...          11\n",
      "4     [Happy, 38th, Birthday, to, my, boo, of, alll,...          12\n",
      "4     [happy, #charitytuesday, @theNSPCC, @SparksCha...           5\n",
      "(1600000, 2)\n",
      "(359, 3)\n"
     ]
    }
   ],
   "source": [
    "tweets = train_data.set_index('Type')\n",
    "tweets['words']= tweets['tweet'].replace(',',' ').replace('.', ' ').str.split()\n",
    "tweets['word_count']= tweets['words'].str.len()\n",
    "tweets = tweets.drop(['tweet'], axis=1)\n",
    "\n",
    "# tweets_test = test_data.set_index('Type')\n",
    "tweets_test=test_data\n",
    "tweets_test['words']= tweets_test['tweet'].str.replace(',',' ').str.replace('.', ' ').str.split()\n",
    "tweets_test['word_count']= tweets_test['words'].str.len()\n",
    "tweets_test = tweets_test.drop(['tweet'], axis=1)\n",
    "print(tweets[:5])\n",
    "print(tweets[-5:])\n",
    "print(tweets.shape)\n",
    "print(tweets_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "for l_words in tweets['words']:\n",
    "    for word in l_words:\n",
    "        vocab[word] =1\n",
    "print(len(vocab))\n",
    "print('Done')\n",
    "\n",
    "total_words_vocab = len(vocab) \n",
    "print(tweets_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_neg=tweets.loc[[0]]\n",
    "tweets_pos=tweets.loc[[4]]\n",
    "# print(tweets_neg[:10]) \n",
    "# print(tweets_pos[0:10])\n",
    "# total_words_neg = tweets_neg['word_count'].sum()\n",
    "# total_words_pos = tweets_pos['word_count'].sum()\n",
    "# print(total_words_neg)\n",
    "# print(total_words_pos)\n",
    "print(tweets_neg.shape)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_neg_words={}\n",
    "dict_neg_words=vocab.copy()\n",
    "print(vocab)\n",
    "print(dict_neg_words)\n",
    "# print(len(dict_neg_words))\n",
    "for l_words in tweets_neg['words']:\n",
    "    word_c =0\n",
    "    for word in l_words:\n",
    "        dict_neg_words[word] +=1\n",
    "count =0\n",
    "for key in dict_neg_words:\n",
    "    count += dict_neg_words[key]\n",
    "print(count)       \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(12040935-1078212)\n",
    "11423286-1078212"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pos_words={}\n",
    "dict_pos_words=vocab.copy()\n",
    "for l_words in tweets_pos['words']:\n",
    "    for word in l_words:\n",
    "        dict_pos_words[word] +=1\n",
    "print(len(dict_pos_words))\n",
    "count =0\n",
    "for key in dict_pos_words:\n",
    "    count += dict_pos_words[key]\n",
    "print(count)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_neg = (tweets_neg.shape[0]+1)/(tweets.shape[0] +2)\n",
    "phi_pos =(tweets_pos.shape[0]+1)/(tweets.shape[0]+2)\n",
    "print(phi_neg)\n",
    "print(phi_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(tweets_neg, tweets_pos, tweets):\n",
    "    count=0\n",
    "    correct =0\n",
    "    total_words_neg = tweets_neg['word_count'].sum()\n",
    "    total_words_pos = tweets_pos['word_count'].sum()\n",
    "    print(total_words_neg)\n",
    "    print(total_words_pos)\n",
    "    for tweet in tweets_neg['words']:\n",
    "        theta_j_neg =0\n",
    "        theta_j_pos =0\n",
    "        for word in tweet:\n",
    "            if word in dict_neg_words:\n",
    "                theta_j_neg += np.log(dict_neg_words[word])\n",
    "                theta_j_pos += np.log(dict_pos_words[word])        \n",
    "        theta_j_neg -= (len(tweet) *np.log(total_words_neg))    \n",
    "        theta_j_pos -= (len(tweet) *np.log(total_words_pos))\n",
    "        prob_y_neg = theta_j_neg + np.log(phi_neg)\n",
    "        prob_y_pos = theta_j_pos + np.log(phi_pos)\n",
    "        if(prob_y_neg > prob_y_pos):\n",
    "            correct += 1\n",
    "    print(correct) \n",
    "    correct_neg = correct\n",
    "    for tweet in tweets_pos['words']:\n",
    "        theta_j_neg =0\n",
    "        theta_j_pos =0\n",
    "        for word in tweet:\n",
    "            if word in dict_neg_words:\n",
    "                theta_j_neg += np.log(dict_neg_words[word])\n",
    "                theta_j_pos += np.log(dict_pos_words[word])\n",
    "        theta_j_neg -= (len(tweet) *np.log(total_words_neg))    \n",
    "        theta_j_pos -= (len(tweet) *np.log(total_words_pos))\n",
    "        prob_y_neg = theta_j_neg + np.log(phi_neg)\n",
    "        prob_y_pos = theta_j_pos + np.log(phi_pos)\n",
    "        if(prob_y_neg < prob_y_pos):\n",
    "            correct += 1   \n",
    "    correct_pos = correct- correct_neg        \n",
    "    print(correct)\n",
    "    print(correct/tweets.shape[0])\n",
    "    return correct_neg, correct_pos\n",
    "    \n",
    "    \n",
    "    \n",
    "start = time.time()\n",
    "check_accuracy(tweets_neg, tweets_pos, tweets)\n",
    "print('time', time.time()- start)\n",
    "print('Done')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_neg_test=tweets_test.loc[tweets_test['Type']==0]\n",
    "tweets_pos_test=tweets_test.loc[tweets_test['Type']==4]\n",
    "total_neg_test = tweets_neg_test.shape[0]\n",
    "total_pos_test = tweets_pos_test.shape[0]\n",
    "print(total_neg_test)\n",
    "print(total_pos_test)\n",
    "start = time.time()\n",
    "correct_neg, correct_pos = check_accuracy(tweets_neg_test, tweets_pos_test, tweets_test)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "wrong_neg = total_neg_test-correct_neg\n",
    "wrong_pos = total_pos_test-correct_pos\n",
    "con_mat = pd.DataFrame([[correct_neg, wrong_pos],[wrong_neg, correct_pos]], columns=['neg_actual', 'pos_actual'], index=['neg_pred', 'pos_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(con_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.(d) stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.strings.StringMethods at 0x7f229921d510>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t['tweet'].str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-ca2ddcb02fde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# tweets['word_count']= tweets['words'].str.len()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# tweets = tweets.drop(['tweet'], axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3470\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3471\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3472\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3474\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3548\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3549\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3550\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3733\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3734\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3735\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3736\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Length of values does not match length of index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "t = train_data\n",
    "print(t.shape[0])\n",
    "t['words']= word_tokenize(str(t['tweet']))\n",
    "# tweets['word_count']= tweets['words'].str.len()\n",
    "# tweets = tweets.drop(['tweet'], axis=1)\n",
    "print(t[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "a=word_tokenize('i have no read the novel on which \"the kite runner\" is based')\n",
    "print(type(a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
