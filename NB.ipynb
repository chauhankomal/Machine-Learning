{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 5.489812612533569\n",
      "(1600000, 2)\n",
      "(359, 2)\n"
     ]
    }
   ],
   "source": [
    "tweets=[]\n",
    "start= time.time()\n",
    "train_data= pd.read_csv('data/training.csv',names = ['Type' , 'id', 'date', 'query','user', 'tweet'], encoding='latin-1')\n",
    "test_data= pd.read_csv('data/test.csv',names = ['Type' , 'id', 'date', 'query','user', 'tweet'], encoding='latin-1')\n",
    "test_data = test_data.drop(['id','date','query','user'], axis=1)\n",
    "train_data= train_data.drop(['id','date','query','user'], axis=1)\n",
    "test_data = test_data.drop(test_data[test_data['Type']==2].index)\n",
    "# print(train_data[:5])\n",
    "# print(train_data[:5])\n",
    "print('time',time.time()-start)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000, 2)\n",
      "(359, 2)\n",
      "                                                  words  word_count\n",
      "Type                                                               \n",
      "0     [@switchfoot, http://twitpic, com/2y1zl, -, Aw...          20\n",
      "0     [is, upset, that, he, can't, update, his, Face...          21\n",
      "0     [@Kenichan, I, dived, many, times, for, the, b...          18\n",
      "0     [my, whole, body, feels, itchy, and, like, its...          10\n",
      "0     [@nationwideclass, no, it's, not, behaving, at...          21\n"
     ]
    }
   ],
   "source": [
    "tweets = train_data.set_index('Type')\n",
    "tweets['words']= tweets['tweet'].str.replace(',',' ').str.replace('.', ' ').str.split()\n",
    "tweets['word_count']= tweets['words'].str.len()\n",
    "tweets = tweets.drop(['tweet'], axis=1)\n",
    "\n",
    "# tweets_test = test_data.set_index('Type')\n",
    "tweets_test=test_data\n",
    "tweets_test['words']= tweets_test['tweet'].str.replace(',',' ').str.replace('.', ' ').str.split()\n",
    "tweets_test = tweets_test.set_index('Type')\n",
    "tweets_test['word_count']= tweets_test['words'].str.len()\n",
    "tweets_test = tweets_test.drop(['tweet'], axis=1)\n",
    "print(tweets.shape)\n",
    "print(tweets_test.shape)\n",
    "print(tweets[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(tweets):\n",
    "    vocab = {}\n",
    "    for l_words in tweets['words']:\n",
    "        for word in l_words:\n",
    "            vocab[word] =1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(tweets,vocab):\n",
    "    tweets_neg=tweets.loc[[0]]\n",
    "    tweets_pos=tweets.loc[[4]]\n",
    "    \n",
    "    phi_neg = (tweets_neg.shape[0]+1)/(tweets.shape[0] +2)\n",
    "    phi_pos =(tweets_pos.shape[0]+1)/(tweets.shape[0]+2)\n",
    "        \n",
    "    dict_neg_words={}\n",
    "    dict_neg_words=vocab.copy()\n",
    "    \n",
    "    for l_words in tweets_neg['words']:\n",
    "        for word in l_words:\n",
    "                dict_neg_words[word] +=1\n",
    "    \n",
    "    dict_pos_words={}\n",
    "    dict_pos_words=vocab.copy()\n",
    "    for l_words in tweets_pos['words']:\n",
    "        for word in l_words:\n",
    "            dict_pos_words[word] +=1\n",
    "    return dict_neg_words, dict_pos_words, phi_neg, phi_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10962723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10345074"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(12040935-1078212)\n",
    "11423286-1078212"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(tweets):\n",
    "    tweets_neg=tweets.loc[[0]]\n",
    "    tweets_pos=tweets.loc[[4]]\n",
    "    count=0\n",
    "    correct =0\n",
    "    total_words_neg = tweets_neg['word_count'].sum()\n",
    "    total_words_pos = tweets_pos['word_count'].sum()\n",
    "    print(total_words_neg)\n",
    "    print(total_words_pos)\n",
    "    total_words_vocab = len(vocab)\n",
    "    for tweet in tweets_neg['words']:\n",
    "        theta_j_neg =0\n",
    "        theta_j_pos =0\n",
    "        for word in tweet:\n",
    "            if word in dict_neg_words:\n",
    "                theta_j_neg += math.log(dict_neg_words[word])\n",
    "            if word in dict_pos_words:  \n",
    "                theta_j_pos += math.log(dict_pos_words[word])\n",
    "        theta_j_neg -= (len(tweet) *math.log(total_words_neg))    \n",
    "        theta_j_pos -= (len(tweet) *math.log(total_words_pos))\n",
    "        prob_y_neg = theta_j_neg + math.log(phi_neg)\n",
    "        prob_y_pos = theta_j_pos + math.log(phi_pos)\n",
    "        if(prob_y_neg > prob_y_pos):\n",
    "            correct += 1\n",
    "    print(correct) \n",
    "    correct_neg = correct\n",
    "    for tweet in tweets_pos['words']:\n",
    "        theta_j_neg =0\n",
    "        theta_j_pos =0\n",
    "        for word in tweet:\n",
    "            if word in dict_neg_words:\n",
    "                theta_j_neg += math.log(dict_neg_words[word])\n",
    "            if word in dict_pos_words:\n",
    "                theta_j_pos += math.log(dict_pos_words[word])\n",
    "        theta_j_neg -= (len(tweet) *math.log(total_words_neg))    \n",
    "        theta_j_pos -= (len(tweet) *math.log(total_words_pos))\n",
    "        prob_y_neg = theta_j_neg + math.log(phi_neg)\n",
    "        prob_y_pos = theta_j_pos + math.log(phi_pos)\n",
    "        if(prob_y_neg < prob_y_pos):\n",
    "            correct += 1   \n",
    "    correct_pos = correct- correct_neg       \n",
    "    print(correct_pos)\n",
    "    print(correct/tweets.shape[0])\n",
    "    return correct_neg, correct_pos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1078262\n",
      "7.92600417137146\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "start= time.time()\n",
    "vocab = create_vocab(tweets)\n",
    "print(len(vocab))\n",
    "dict_neg_words, dict_pos_words, phi_neg, phi_pos= create_dict(tweets,vocab)\n",
    "print(time.time()-start)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10962673\n",
      "10345024\n",
      "708204\n",
      "652067\n",
      "0.850169375\n",
      "time 16.427900075912476\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "check_accuracy(tweets)\n",
    "print('time', time.time()- start)\n",
    "print('Done')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2694\n",
      "2633\n",
      "150\n",
      "141\n",
      "0.8105849582172702\n",
      "0.006552696228027344\n"
     ]
    }
   ],
   "source": [
    "total_neg_test = tweets_test.loc[[0]].shape[0]\n",
    "total_pos_test = tweets_test.loc[[4]].shape[0]\n",
    "start = time.time()\n",
    "correct_neg, correct_pos = check_accuracy(tweets_test)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "wrong_neg = total_neg_test-correct_neg\n",
    "wrong_pos = total_pos_test-correct_pos\n",
    "con_mat = pd.DataFrame([[correct_neg, wrong_pos],[wrong_neg, correct_pos]], columns=['neg_actual', 'pos_actual'], index=['neg_pred', 'pos_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          neg_actual  pos_actual\n",
      "neg_pred         150          41\n",
      "pos_pred          27         141\n"
     ]
    }
   ],
   "source": [
    "print(con_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.(d) stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = train_data\n",
    "# print(t.shape[0])\n",
    "# start = time.time()\n",
    "# t['words']=(t['tweet'].str.lower()).map(word_tokenize)\n",
    "# # tweets['word_count']= tweets['words'].str.len()\n",
    "# # tweets = tweets.drop(['tweet'], axis=1)\n",
    "# print(time.time()- start)\n",
    "# print(t[:12])\n",
    "# print('Done')\n",
    "# t = t.drop(['tweet'], axis=1)\n",
    "# t.to_pickle('df_tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sep(str):\n",
    "    temp= str.lower().replace(',',' ').replace('.',' ').split()\n",
    "    res =[stemming.stem(word) for word in temp if word not in stop_words and '@' not in word]        \n",
    "    return res        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219.66432762145996\n",
      "Done\n",
      "                                                  words  word_count\n",
      "Type                                                               \n",
      "0     [http://twitpic, com/2y1zl, -, awww, that', bu...          13\n",
      "0     [upset, can't, updat, facebook, text, might, c...          12\n",
      "0     [dive, mani, time, ball, manag, save, 50%, res...          10\n",
      "0                [whole, bodi, feel, itchi, like, fire]           6\n",
      "0                  [behav, i'm, mad, here?, can't, see]           6\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "tweets = train_data.set_index('Type')\n",
    "start = time.time()\n",
    "tweets['words']= tweets['tweet'].map(sep)\n",
    "tweets['word_count']= tweets['words'].str.len()\n",
    "tweets = tweets.drop(['tweet'], axis=1)\n",
    "print(time.time()-start)\n",
    "print('Done')\n",
    "print(tweets[:5])\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6138839\n",
      "5947023\n",
      "663939\n",
      "626842\n",
      "0.806738125\n",
      "time 11.838244199752808\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "vocab = create_vocab(tweets)\n",
    "dict_neg_words, dict_pos_words, phi_neg, phi_pos= create_dict(tweets)\n",
    "start = time.time()\n",
    "check_accuracy(tweets)\n",
    "print('time', time.time()- start)\n",
    "print('Done')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2694\n",
      "2633\n",
      "116\n",
      "145\n",
      "0.7270194986072424\n",
      "0.0115966796875\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "correct_neg, correct_pos = check_accuracy(tweets_test)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (e) TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tfidfVectorizer=TfidfVectorizer(use_idf=True)\n",
    " \n",
    "tfidf_vectors=tfidfVectorizer.fit_transform(train_data['tweet'].tolist())\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000, 684358)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vectors.shape)\n",
    "# print(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_vector=tfidf_vectors[0]\n",
    "# df = pd.DataFrame(first_vector.T.todense(), index=tfidfVectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "# df.sort_values(by=[\"tfidf\"])\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_data['Type'].toarray().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model=GaussianNB()\n",
    "# temp = tfidf_vectors.tocoo()\n",
    "# print(temp.shape)\n",
    "# t = [temp.col.tolist(), temp.data.tolist()]\n",
    "# print(len(t))\n",
    "nb_model.fit(tfidf_vectors.todense(),train_data['Type'].tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
